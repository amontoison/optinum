var documenterSearchIndex = {"docs":
[{"location":"fct_index.html#Documentation-des-fonctions-1","page":"Documentation des fonctions","title":"Documentation des fonctions","text":"","category":"section"},{"location":"fct_index.html#","page":"Documentation des fonctions","title":"Documentation des fonctions","text":"Documentation de toute les fonctions du package Optinum","category":"page"},{"location":"fct_index.html#","page":"Documentation des fonctions","title":"Documentation des fonctions","text":"Pages = [\"fct_index.md\"]","category":"page"},{"location":"fct_index.html#","page":"Documentation des fonctions","title":"Documentation des fonctions","text":"Modules = [Optinum]\nOrder   = [:function, :type]","category":"page"},{"location":"fct_index.html#Optinum.Gradient_Conjugue_Tronque-NTuple{4,Any}","page":"Documentation des fonctions","title":"Optinum.Gradient_Conjugue_Tronque","text":"Minimise le problème : min q(s) avec s deltak                                 pour l'itération k de l'algorithme des regions de confiance\n\nSyntaxe\n\nsk = Gradient_Conjugue_Tronque(fk,gradfk,hessfk,option)\n\nEntrées :\n\n* fk               : la fonction à minimiser appliqué au point xk\n* gradfk           : le gradient de la fonction f appliqué au point xk\n* hessfk           : la Hessienne de la fonction f appliqué au point xk\n* options          :\n        * delta            : le rayon de la région de confiance\n        * max_iter         : le nombre maximal d'iterations\n        * tol              : la tolerance pour la condition d'arrêt sur le gradient\n\nSorties:\n\n* s                : le pas s qui approche la solution du problème : min q(s) avec ||s||< deltak\n\nExemple d'appel:\n\ninclude(\"fonctions_de_tests.jl\")\ns = Gradient_Conjugue_Tronque(fct1([10;0;3]),grad_fct1([10;0;3]),hess_fct1([10;0;3]),1,[10;0;3],100,1e-15)\n\n\n\n\n\n","category":"method"},{"location":"fct_index.html#Optinum.Lagrangien_Augmente-Tuple{Any,Function,Function,Function,Function,Function,Function,Function,Function,Function,Any,Any}","page":"Documentation des fonctions","title":"Optinum.Lagrangien_Augmente","text":"Resolution des problèmes de minimisation sous contraintes d'égalités\n\nSyntaxe\n\nLagrangien_Augmente(algorithme_sans_contrainte,fonc,contrainte,gradfonc,hessfonc,gradcontrainte,\n\t\t\t\t\thesscontrainte,normcontrainte,jaccontrainte,phi,x0,option)\n\nEntrées\n\n* algorithme_sans_contrainte : l'indice inqiquant l'algorithme sans contraintes à utiliser\n* \"newton\" \t\t\t\t\t : pour utiliser l'algo de Newton\n* \"cauchy\" \t\t\t\t\t : pour utiliser l'algo de Région de confiance avec cauchy\n* \"gct\" \t\t\t\t \t : pour utiliser l'algo de Région de confiance avec gradient conjugué tronqué\n* fonc \t\t\t\t\t\t : la fonction à minimiser\n* contrainte \t\t\t\t : la contrainte [x est dans le domaine des contraintes ssi c(x)==0]\n* gradfonc\t\t\t\t\t : le gradient de la fonction\n* hessfonc \t\t\t\t\t : la hessienne de la fonction\n* gradcontrainte \t\t\t : le gradient de la contrainte contrainte\n* hesscontrainte \t\t\t : la hessienne de la contrainte\n* nomrcontrainte \t\t\t : la norme de la contrainte\n* jaccontrainte \t\t\t : le jacobien de la contrainte\n* x0 \t\t\t\t\t\t : le point du départ\n* options      \t\t\t\t:\n\t\t* epsilon \t\t\t\t\t : utilisé dans les critéres d'arrêt\n\t\t* tol \t\t\t\t\t\t : utilisé dans les critéres d'arrêt\n\t\t* itermax \t\t\t\t\t : nombre maximal d'itération dans la boucle principale\n\t\t* lambda0,mu0,tho \t\t\t : valeurs initiales des variables de l'algorithme\n\nSorties\n\n* xmin \t\t\t\t:Le minumum du problème avec contraintes\n* fxmin \t\t\t: l'image de xmin par la fonction objectif\n* flag \t\t\t\t: indicateur du déroulement de l'algorithme\n* niters \t\t\t: nombre d'itérations\n\n \t 0 : Convergence\n \t 1 : nombre maximal d'itération atteint\n \t-1 : une erreur s'est produite\n\nExemple d'appel\n\n\n\n\n\n\n\n","category":"method"},{"location":"fct_index.html#Optinum.Regions_De_Confiance-Tuple{Any,Function,Function,Function,Any,Any}","page":"Documentation des fonctions","title":"Optinum.Regions_De_Confiance","text":"Minimise une fonction en utilisant l'algorithme des régions de confiance avec     - le pas de Couchy ou     - le pas issu de l'algorithme du gradient conjugue tronqué\n\nSyntaxe\n\nxk, nb_iters, f(xk), flag = Regions_De_Confiance(algo,f,gradf,hessf,x0,option)\n\nEntrées :\n\n* algo        : string indicant la méthode à utiliser pour calculer le pas\n                    - \"gct\" : pour l'algorithme du gradient conjugué tronqué\n                    - \"cauchy\": pour le pas de Cauchy\n* f           : la fonction à minimiser\n* gradf       : le gradient de la fonction f\n* hessf       : la hessiene de la fonction à minimiser\n* x0          : point de dapart\n* options     : \n            * deltaMax, 0 < gamma1 < 1, 1 < gamma2, 0 < eta1, eta1 < eta2 < 1  : utile pour les m-à-j de la region de confiance\n            * delta0      : le rayon de départ\n            * max_iter    : le nombre maximale d'iterations\n            * tol         : la tolérence pour les critères d'arrêt\n\nSorties:\n\n* x_min    : le point minimisant la fonction f\n* fx_min   : la valeur minimale de la fonction f\n* flag     : entier indiquant le critère sur lequel le programme à arrêter\n* nb_iters : le nombre d'iteration qu'à fait le programme\n\n        \t    0 : Convergence\n        \t    1 : stagnation du x\n        \t    2 : stagnation du f\n        \t    3 : nombre maximal d'itération dépassé\n\nExemple d'appel\n\n\n\n\n\n\n\n","category":"method"},{"location":"mise_en_place.html#Mise-en-place-de-Julia-1","page":"Mise en place","title":"Mise en place de Julia","text":"","category":"section"},{"location":"mise_en_place.html#","page":"Mise en place","title":"Mise en place","text":"Les étapes permettant d'installer Julia ainsi que les programmes nécessaires afin d'obtenir une interface de type IDE avec l'éditeur Atom sont décrites dans cette section.","category":"page"},{"location":"mise_en_place.html#Installation-de-Julia-1","page":"Mise en place","title":"Installation de Julia","text":"","category":"section"},{"location":"mise_en_place.html#","page":"Mise en place","title":"Mise en place","text":"Le langage de programmation Julia peut être téléchargée sur le site web Download Julia. La version v1 ou une version plus récente doit être téléchargée.","category":"page"},{"location":"mise_en_place.html#Lancement-des-tests-1","page":"Mise en place","title":"Lancement des tests","text":"","category":"section"},{"location":"mise_en_place.html#","page":"Mise en place","title":"Mise en place","text":"Dans le dossier tests faire :","category":"page"},{"location":"mise_en_place.html#","page":"Mise en place","title":"Mise en place","text":"include(\"runtests.jl\") ","category":"page"},{"location":"mise_en_place.html#lancement-d'un-seul-test-1","page":"Mise en place","title":"lancement d'un seul test","text":"","category":"section"},{"location":"mise_en_place.html#","page":"Mise en place","title":"Mise en place","text":"faire include du fichier contenant la fonction de test  et exécuter cette fonction avec le paramétre 'true' pour l'affichage","category":"page"},{"location":"mise_en_place.html#exemple-(tester-l'algorithme-du-Lagrangien-augmenté-)-:-1","page":"Mise en place","title":"exemple (tester l'algorithme du Lagrangien augmenté ) :","text":"","category":"section"},{"location":"mise_en_place.html#","page":"Mise en place","title":"Mise en place","text":"include(\"test_Lagrangien_Aug.jl\") ","category":"page"},{"location":"mise_en_place.html#","page":"Mise en place","title":"Mise en place","text":"test_Lagrangien_Aug(true) ","category":"page"},{"location":"generate_pdf.html#Génération-d'un-fichier-pdf-1","page":"Génération du rapport","title":"Génération d'un fichier pdf","text":"","category":"section"},{"location":"generate_pdf.html#","page":"Génération du rapport","title":"Génération du rapport","text":"Le package Weave permet de générer un rapport sous format pdf d'un script Julia.","category":"page"},{"location":"generate_pdf.html#","page":"Génération du rapport","title":"Génération du rapport","text":"Le fichier gabarit_rapport.jl est un gabarit de rapport. Après avoir édité votre script, vous pouvez publier votre script en format pdf (avec pdflatex) avec la commande:","category":"page"},{"location":"generate_pdf.html#","page":"Génération du rapport","title":"Génération du rapport","text":"using Weave\nweave(\"gabarit_rapport.jl\", doctype = \"md2pdf\")","category":"page"},{"location":"generate_pdf.html#","page":"Génération du rapport","title":"Génération du rapport","text":"ou en format HTML avec la commande:","category":"page"},{"location":"generate_pdf.html#","page":"Génération du rapport","title":"Génération du rapport","text":"using Weave\nweave(\"gabarit_rapport.jl\", doctype = \"md2html\")","category":"page"},{"location":"generate_pdf.html#","page":"Génération du rapport","title":"Génération du rapport","text":"Il est important d'imprimer ensuite le fichier HTML en format pdf pour la remise sur Moodle.","category":"page"},{"location":"generate_pdf.html#","page":"Génération du rapport","title":"Génération du rapport","text":"warning: Avertissement\nLes lignes de code précédentes ne doivent pas se retrouver dans votre script. Vous devez les éxécuter dans la console de Julia en vous assurant d'être dans le dossier contenant le fichier gabarit_rapport.jl.","category":"page"},{"location":"Sujet.html#Sujet-TP-Projet-Optimisation-numérique-2A-ENSEEIHT-1","page":"Sujet","title":"Sujet TP-Projet Optimisation numérique 2A-ENSEEIHT","text":"","category":"section"},{"location":"Sujet.html#","page":"Sujet","title":"Sujet","text":"La première partie de ce TP-projet concerne les problèmes d’optimisation sans contraintes. On étudie la méthode de Newton et sa globalisation par l’algorithme des régions de confiance. La résolution du sous-problème des régions de confiance sera réalisée de deux façons, soit à l’aide du point de Cauchy, soit par l’algorithme du Gra- dient Conjugué Tronqué. La seconde partie du projet exploite la partie précédente pour résoudre des problèmes d’optimisation avec contraintes par l’algorithme du Lagrangien augmenté.","category":"page"},{"location":"Sujet.html#Optimisation-sans-contraintes-1","page":"Sujet","title":"Optimisation sans contraintes","text":"","category":"section"},{"location":"Sujet.html#","page":"Sujet","title":"Sujet","text":"Dans cette partie, on s’intéresse à la résolution du problème","category":"page"},{"location":"Sujet.html#","page":"Sujet","title":"Sujet","text":"hspace*25 min _x in mathbbR^m f(x)  où la fonction f est de classe C^2 sur R^n . On cherche donc à exploiter l’information fournie par ses dérivées première et seconde, que l’on représente en tout point x par le vecteur gradient nabla f (x) in R^net la matrice Hessienne nabla^2 f (x) in R^nxn.","category":"page"},{"location":"Sujet.html#","page":"Sujet","title":"Sujet","text":"1/- Algorithme de Newton local","category":"page"},{"location":"Sujet.html#","page":"Sujet","title":"Sujet","text":"2/- Région de confiance-Partie 1: Avec le pas de cauchy","category":"page"},{"location":"Sujet.html#","page":"Sujet","title":"Sujet","text":"3/- Région de confiance-Partie 2: Avec l'agorithme du Gradient Conjugué Tronqué","category":"page"},{"location":"index.html#Optinum-Optimisation-Numérique-1","page":"Accueil","title":"Optinum - Optimisation Numérique","text":"","category":"section"},{"location":"index.html#","page":"Accueil","title":"Accueil","text":"Le package Optinum contient tous les algorithmes vus en cours d'Optimisation de l'École INP-ENSEEIHT.","category":"page"},{"location":"Algorithme_de_newton.html#Algorithme-de-Newton-local-1","page":"Algorithme de Newton","title":"Algorithme de Newton local","text":"","category":"section"},{"location":"Algorithme_de_newton.html#Principe-1","page":"Algorithme de Newton","title":"Principe","text":"","category":"section"},{"location":"Algorithme_de_newton.html#","page":"Algorithme de Newton","title":"Algorithme de Newton","text":"hspace*07 La fonction f étant C^2 , on peut remplacer f au voisinage de l’itéré courant ``x_{k} par son développement de Taylor au second ordre, soit :","category":"page"},{"location":"Algorithme_de_newton.html#","page":"Algorithme de Newton","title":"Algorithme de Newton","text":"hspace*07 f(y) sim q(y)=fleft(x_kright)+nabla fleft(x_kright)^Tleft(y-x_kright)+frac12left(y-x_kright)^T nabla^2 fleft(x_kright)left(y-x_kright) On choisit alors comme point x_k+1 le minimum de la quadratique q lorsqu’il existe et est unique, ce qui n’est le cas que si nabla^2 f (x) est définie positive. Or le minimum de q est réalisé par x k+1 solution de : nabla^2 f (x_k+1) = 0, soit : hspace*17 nabla fleft(x_kright)+nabla^2 fleft(x_kright)left(x_k+1-x_kright)=0  ou encore, en supposant que nabla^2 f (x_k)est définie positive :","category":"page"},{"location":"Algorithme_de_newton.html#","page":"Algorithme de Newton","title":"Algorithme de Newton","text":"hspace*17 x_k+1=x_k-nabla^2 fleft(x_kright)^-1 nabla fleft(x_kright) hspace*07La méthode ne doit cependant jamais être appliquée en utilisant une inversion de la matrice Hessienne (qui peut être de très grande taille et mal conditionnée), mais plutôt en utilisant :","category":"page"},{"location":"Algorithme_de_newton.html#","page":"Algorithme de Newton","title":"Algorithme de Newton","text":"hspace*17 x_k+1=x_k+d_k où d_k est l’unique solution du système linéaire","category":"page"},{"location":"Algorithme_de_newton.html#","page":"Algorithme de Newton","title":"Algorithme de Newton","text":"hspace*17 nabla^2 fleft(x_kright) d_k=-nabla fleft(x_kright)  d_k étant appelée direction de Newton. hspace*07 Cette méthode est bien définie si à chaque itération, la matrice hessienne nabla^2 f (x_k) est définie positive : ceci est vrai en particulier au voisinage de la solutionx_k cherchée si on suppose que nabla^2 f (x_*) est définie positive (par continuité de nabla^2 f).","category":"page"},{"location":"Algorithme_de_newton.html#Algorithme-1","page":"Algorithme de Newton","title":"Algorithme","text":"","category":"section"},{"location":"Algorithme_de_newton.html#Données:-1","page":"Algorithme de Newton","title":"Données:","text":"","category":"section"},{"location":"Algorithme_de_newton.html#","page":"Algorithme de Newton","title":"Algorithme de Newton","text":"f , x_0 première approximation de la solution cherchée, epsilon  0 précision demandée.","category":"page"},{"location":"Algorithme_de_newton.html#Sorties-1","page":"Algorithme de Newton","title":"Sorties","text":"","category":"section"},{"location":"Algorithme_de_newton.html#","page":"Algorithme de Newton","title":"Algorithme de Newton","text":"une approximation de la solution du problème min _x in mathbbR^m f(x).","category":"page"},{"location":"Algorithme_de_newton.html#.Tant-que-le-test-de-convergence-est-non-satisfait-1","page":"Algorithme de Newton","title":"1.Tant que le test de convergence est non satisfait","text":"","category":"section"},{"location":"Algorithme_de_newton.html#","page":"Algorithme de Newton","title":"Algorithme de Newton","text":"a. Calculer d k solution du système : nabla^2 f (x_k) d_k = nabla f (x_k),   b. Mise à jour : x_k+1 = x_k+ d_k  k = k + 1,","category":"page"},{"location":"Algorithme_de_newton.html#Retourner-x_{k}.-1","page":"Algorithme de Newton","title":"Retourner x_k.","text":"","category":"section"}]
}
